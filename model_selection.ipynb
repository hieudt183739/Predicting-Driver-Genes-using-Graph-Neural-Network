{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
    "from sklearn import metrics\n",
    "from model_all import Net\n",
    "from sklearn import linear_model\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import load_data\n",
    "from tqdm import tqdm\n",
    "from statistics import mean\n",
    "\n",
    "\n",
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pos = pd.read_csv('data/luad/pos.txt', header=None)\n",
    "neg = pd.read_csv('data/luad/neg.txt', header=None)\n",
    "index = pd.concat([pos,neg]).reset_index(drop=True).to_numpy().reshape(-1)   # index gene co nhan trong file gene_names.txt\n",
    "label = np.zeros(2366)\n",
    "label[0:179] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chia tập dữ liệu 80% train và 20% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train, index_test, label_train, label_test = train_test_split(index, label, test_size=0.2, shuffle=True, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu siêu tham số bằng Cross-Validation. Chia tập train thành 5 Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa số ngưỡng quyết định liên kết Similarity Gene-Gene \n",
    "Chọn ngưỡng quyết định liên kết giữa 2 gene trong similarity gene-gene graph là [0.7,0.8,0.9]  \n",
    "Cố định mô hình với các siêu tham số mặc định:  \n",
    "Epoch = 1000, w1 = 0.1, w2 = 0.01, lr = 0.001, weight decay = 0, mô hình phân loại Logistic Regression  \n",
    "== > Chọn ngưỡng = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 1000\n",
    "w1 = 0.1\n",
    "w2 = 0.01\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0     \n",
    "\n",
    "threshold = [ 0.7, 0.8, 0.9]\n",
    "\n",
    "for t in threshold:\n",
    "    print('Similarity Threshold = ', t)\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "\n",
    "    network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = t)\n",
    "    for tr, val in kf.split(index_train,label_train):\n",
    "        model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1).to(device)     # hop=1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "            pred, pred1, r_loss, _ = model()\n",
    "            loss = F.binary_cross_entropy_with_logits(pred[index_train[tr]], Y[index_train[tr]])\n",
    "            loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[tr]], Y[index_train[tr]])\n",
    "            loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            _, _, _, x = model()\n",
    "\n",
    "            # logistic regression model\n",
    "            train_x = (x[index_train[tr]]).cpu().detach().numpy()\n",
    "            train_y = Y[index_train[tr]].cpu().numpy()\n",
    "\n",
    "            val_x = x[index_train[val]].cpu().detach().numpy()\n",
    "            val_y = Y[index_train[val]].cpu().numpy().reshape(-1)\n",
    "\n",
    "            regr = linear_model.LogisticRegression(max_iter=10000)\n",
    "            regr.fit(train_x, train_y.ravel())\n",
    "            pre = regr.predict_proba(val_x)\n",
    "            pre = pre[:,1]\n",
    "            y_pred = regr.predict(val_x)\n",
    "\n",
    "            auprc = metrics.average_precision_score(val_y, pre)\n",
    "            auc = metrics.roc_auc_score(val_y, pre)   \n",
    "\n",
    "            AUC.append(auc)\n",
    "            AUPRC.append(auprc)\n",
    "            print(auc)\n",
    "            print(auprc)\n",
    "        \n",
    "\n",
    "    np.savetxt(f\"result/threshold/AUC_test_{t}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/threshold/AUPRC_test_{t}.txt\", AUPRC, delimiter=\"\\t\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa số lương EPOCH\n",
    "Chạy EPOCH = 4000   \n",
    "Cố định siêu tham số đã xác định: threshold = 0.7    \n",
    "Các siêu tham số khác đặt mặc định, w1 = 0.1, w2 = 0.01, lr = 0.001, weight decay = 0   \n",
    "Mô hình phân loại Logistic Regression   \n",
    "== > Chọn EPOCH = 1000 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 4000\n",
    "w1 = 0.1\n",
    "w2 = 0.01\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0     \n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "LOSS = []\n",
    "\n",
    "network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = threshold)\n",
    "for tr, val in kf.split(index_train,label_train):\n",
    "    LOSS = []\n",
    "    \n",
    "    model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1).to(device)     # hop=1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "        pred, pred1, r_loss, _ = model()\n",
    "        loss = F.binary_cross_entropy_with_logits(pred[index_train[tr]], Y[index_train[tr]])\n",
    "        loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[tr]], Y[index_train[tr]])\n",
    "        loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_loss = F.binary_cross_entropy_with_logits(pred[index_train[val]], Y[index_train[val]])\n",
    "        val_loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[val]], Y[index_train[val]])\n",
    "        val_loss = val_loss + w1 * val_loss1 + w2 * r_loss\n",
    "        LOSS.append(val_loss.item())\n",
    "\n",
    "    # Lưu lại LOSS  \n",
    "    with open('result/epoch/embedding_loss.txt', 'w') as file:\n",
    "        for item in LOSS:\n",
    "            file.write(f\"{item}\\n\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa learning rate\n",
    "Cố định EPOCH = 1000, threshold = 0.7.  \n",
    "learning_rate = [0.0005, 0.001, 0.002]  \n",
    "tham số w1 = 0.1, w2 = 0.001  \n",
    "== > Learning Rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 1000\n",
    "w1 = 0.1\n",
    "w2 = 0.01\n",
    "learning_rate = [0.0005, 0.001, 0.002]\n",
    "weight_decay = 0     \n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "for l in learning_rate:\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "    network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = threshold)\n",
    "    \n",
    "    for tr, val in kf.split(index_train,label_train):\n",
    "        model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1).to(device)     # hop=1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=l)\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "            pred, pred1, r_loss, _ = model()\n",
    "            loss = F.binary_cross_entropy_with_logits(pred[index_train[tr]], Y[index_train[tr]])\n",
    "            loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[tr]], Y[index_train[tr]])\n",
    "            loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            _, _, _, x = model()\n",
    "\n",
    "            # logistic regression model\n",
    "            train_x = (x[index_train[tr]]).cpu().detach().numpy()\n",
    "            train_y = Y[index_train[tr]].cpu().numpy()\n",
    "\n",
    "            val_x = x[index_train[val]].cpu().detach().numpy()\n",
    "            val_y = Y[index_train[val]].cpu().numpy().reshape(-1)\n",
    "\n",
    "            regr = linear_model.LogisticRegression(max_iter=10000)\n",
    "            regr.fit(train_x, train_y.ravel())\n",
    "            pre = regr.predict_proba(val_x)\n",
    "            pre = pre[:,1]\n",
    "            y_pred = regr.predict(val_x)\n",
    "\n",
    "            auprc = metrics.average_precision_score(val_y, pre)\n",
    "            auc = metrics.roc_auc_score(val_y, pre)   \n",
    "\n",
    "            AUC.append(auc)\n",
    "            AUPRC.append(auprc)\n",
    "        \n",
    "    print(f'Voi learning_rate = {l}: mean(AUC) = {mean(AUC)}')\n",
    "    print(f'Voi learning_rate = {l}: mean(AUPRC) = {mean(AUPRC)}')\n",
    "    \n",
    "    np.savetxt(f\"result/learning_rate/AUC_test_{l}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/learning_rate/AUPRC_test_{l}.txt\", AUPRC, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa w1\n",
    "Cố định Epoch = 1000, t=0.7, w2 = 0.01, weight_decay = 0, learning_rate = 0.01. Tìm w1 = [0.05, 0.1, 0.2, 0.3]\n",
    "===> chon w1 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 1000\n",
    "W1 = [0.005, 0.1, 0.2, 0.3]\n",
    "w2 = 0.01\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0     \n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "for w1 in W1:\n",
    "    print('+ w1 = ', w1)\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "    network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = threshold)\n",
    "\n",
    "    num_fold = -1\n",
    "    for tr, val in kf.split(index_train,label_train):\n",
    "        num_fold = num_fold + 1\n",
    "        print(f\"Xet Fold {num_fold}:\")\n",
    "        \n",
    "        LOSS = []\n",
    "        model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1).to(device)     # hop=1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "            pred, pred1, r_loss, _ = model()\n",
    "            loss = F.binary_cross_entropy_with_logits(pred[index_train[tr]], Y[index_train[tr]])\n",
    "            loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[tr]], Y[index_train[tr]])\n",
    "            loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            val_loss = F.binary_cross_entropy_with_logits(pred[index_train[val]], Y[index_train[val]])\n",
    "            val_loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[val]], Y[index_train[val]])\n",
    "            val_loss = val_loss + w1 * val_loss1 + w2 * r_loss\n",
    "            LOSS.append(val_loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            _, _, _, x = model()\n",
    "\n",
    "            # logistic regression model\n",
    "            train_x = (x[index_train[tr]]).cpu().detach().numpy()\n",
    "            train_y = Y[index_train[tr]].cpu().numpy()\n",
    "\n",
    "            val_x = x[index_train[val]].cpu().detach().numpy()\n",
    "            val_y = Y[index_train[val]].cpu().numpy().reshape(-1)\n",
    "\n",
    "            regr = linear_model.LogisticRegression(max_iter=10000)\n",
    "            regr.fit(train_x, train_y.ravel())\n",
    "            pre = regr.predict_proba(val_x)\n",
    "            pre = pre[:,1]\n",
    "            y_pred = regr.predict(val_x)\n",
    "\n",
    "            auprc = metrics.average_precision_score(val_y, pre)\n",
    "            auc = metrics.roc_auc_score(val_y, pre)   \n",
    "\n",
    "            AUC.append(auc)\n",
    "            AUPRC.append(auprc)\n",
    "        np.savetxt(f\"result/w1/embedding_loss_{w1}_fold{num_fold}.txt\", LOSS, delimiter=\"\\t\")\n",
    "        \n",
    "    print(f'Voi w1 = {w1}: mean(AUC) = {mean(AUC)}')\n",
    "    print(f'Voi w1 = {w1}: mean(AUPRC) = {mean(AUPRC)}')\n",
    "\n",
    "    np.savetxt(f\"result/w1/AUC_test_{w1}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/w1/AUPRC_test_{w1}.txt\", AUPRC, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa w2\n",
    "Cố định Epoch = 1000, t=0.7, w1 = 0.1, weight_decay = 0, learning_rate = 0.001. Tìm W2 = [0.005, 0.01, 0.015, 0.02, 0.025]\n",
    "===> chon w1 = 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 1000\n",
    "w1 = 0.1\n",
    "W2 = [0.005, 0.01, 0.015, 0.02, 0.025]\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0     \n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "for w2 in W2:\n",
    "    print('+ w2 = ', w2)\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "    network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = threshold)\n",
    "\n",
    "    num_fold = -1\n",
    "    for tr, val in kf.split(index_train,label_train):\n",
    "        num_fold = num_fold + 1\n",
    "        print(f\"Xet Fold {num_fold}:\")\n",
    "        \n",
    "        LOSS = []\n",
    "        model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1).to(device)     # hop=1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "            pred, pred1, r_loss, _ = model()\n",
    "            loss = F.binary_cross_entropy_with_logits(pred[index_train[tr]], Y[index_train[tr]])\n",
    "            loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[tr]], Y[index_train[tr]])\n",
    "            loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            val_loss = F.binary_cross_entropy_with_logits(pred[index_train[val]], Y[index_train[val]])\n",
    "            val_loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[val]], Y[index_train[val]])\n",
    "            val_loss = val_loss + w1 * val_loss1 + w2 * r_loss\n",
    "            LOSS.append(val_loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            _, _, _, x = model()\n",
    "\n",
    "            # logistic regression model\n",
    "            train_x = (x[index_train[tr]]).cpu().detach().numpy()\n",
    "            train_y = Y[index_train[tr]].cpu().numpy()\n",
    "\n",
    "            val_x = x[index_train[val]].cpu().detach().numpy()\n",
    "            val_y = Y[index_train[val]].cpu().numpy().reshape(-1)\n",
    "\n",
    "            regr = linear_model.LogisticRegression(max_iter=10000)\n",
    "            regr.fit(train_x, train_y.ravel())\n",
    "            pre = regr.predict_proba(val_x)\n",
    "            pre = pre[:,1]\n",
    "            y_pred = regr.predict(val_x)\n",
    "\n",
    "            auprc = metrics.average_precision_score(val_y, pre)\n",
    "            auc = metrics.roc_auc_score(val_y, pre)   \n",
    "\n",
    "            AUC.append(auc)\n",
    "            AUPRC.append(auprc)\n",
    "        np.savetxt(f\"result/w2/embedding_loss_{w2}_fold{num_fold}.txt\", LOSS, delimiter=\"\\t\")\n",
    "        \n",
    "    print(f'Voi w2 = {w2}: mean(AUC) = {mean(AUC)}')\n",
    "    print(f'Voi w2 = {w2}: mean(AUPRC) = {mean(AUPRC)}')\n",
    "\n",
    "    np.savetxt(f\"result/w2/AUC_test_{w2}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/w2/AUPRC_test_{w2}.txt\", AUPRC, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa Weight decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 1000\n",
    "w1 = 0.1\n",
    "w2 = 0.015\n",
    "learning_rate = 0.001\n",
    "weight_decay = [0.01, 0.02, 0.03]     \n",
    "\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "for wc in weight_decay:\n",
    "    print('+ wc = ', wc)\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "    network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = threshold)\n",
    "\n",
    "    num_fold = -1\n",
    "    for tr, val in kf.split(index_train,label_train):\n",
    "        num_fold = num_fold + 1\n",
    "        print(f\"Xet Fold {num_fold}:\")\n",
    "        \n",
    "        LOSS = []\n",
    "        model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1).to(device)     # hop=1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=wc)\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "            pred, pred1, r_loss, _ = model()\n",
    "            loss = F.binary_cross_entropy_with_logits(pred[index_train[tr]], Y[index_train[tr]])\n",
    "            loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[tr]], Y[index_train[tr]])\n",
    "            loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            val_loss = F.binary_cross_entropy_with_logits(pred[index_train[val]], Y[index_train[val]])\n",
    "            val_loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[val]], Y[index_train[val]])\n",
    "            val_loss = val_loss + w1 * val_loss1 + w2 * r_loss\n",
    "            LOSS.append(val_loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            _, _, _, x = model()\n",
    "\n",
    "            # logistic regression model\n",
    "            train_x = (x[index_train[tr]]).cpu().detach().numpy()\n",
    "            train_y = Y[index_train[tr]].cpu().numpy()\n",
    "\n",
    "            val_x = x[index_train[val]].cpu().detach().numpy()\n",
    "            val_y = Y[index_train[val]].cpu().numpy().reshape(-1)\n",
    "\n",
    "            regr = linear_model.LogisticRegression(max_iter=10000)\n",
    "            regr.fit(train_x, train_y.ravel())\n",
    "            pre = regr.predict_proba(val_x)\n",
    "            pre = pre[:,1]\n",
    "            y_pred = regr.predict(val_x)\n",
    "\n",
    "            auprc = metrics.average_precision_score(val_y, pre)\n",
    "            auc = metrics.roc_auc_score(val_y, pre)   \n",
    "\n",
    "            AUC.append(auc)\n",
    "            AUPRC.append(auprc)\n",
    "        np.savetxt(f\"result/wc/embedding_loss_{wc}_fold{num_fold}.txt\", LOSS, delimiter=\"\\t\")\n",
    "        \n",
    "    print(f'Voi wc = {wc}: mean(AUC) = {mean(AUC)}')\n",
    "    print(f'Voi wc = {wc}: mean(AUPRC) = {mean(AUPRC)}')\n",
    "\n",
    "    np.savetxt(f\"result/wc/AUC_test_{wc}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/wc/AUPRC_test_{wc}.txt\", AUPRC, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 1000\n",
    "w1 = 0.1\n",
    "w2 = 0.015\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0 \n",
    "Alpha = [0.1, 0.15, 0.2, 0.25]\n",
    "threshold = 0.7\n",
    "\n",
    "\n",
    "for alpha in Alpha:\n",
    "    print('+ alpha = ', alpha)\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "    network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = threshold)\n",
    "\n",
    "    num_fold = -1\n",
    "    for tr, val in kf.split(index_train,label_train):\n",
    "        num_fold = num_fold + 1\n",
    "        print(f\"Xet Fold {num_fold}:\")\n",
    "        \n",
    "        LOSS = []\n",
    "        model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1, alpha=alpha).to(device)     # hop=1\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        model.train()\n",
    "        for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "            pred, pred1, r_loss, _ = model()\n",
    "            loss = F.binary_cross_entropy_with_logits(pred[index_train[tr]], Y[index_train[tr]])\n",
    "            loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[tr]], Y[index_train[tr]])\n",
    "            loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            val_loss = F.binary_cross_entropy_with_logits(pred[index_train[val]], Y[index_train[val]])\n",
    "            val_loss1 = F.binary_cross_entropy_with_logits(pred1[index_train[val]], Y[index_train[val]])\n",
    "            val_loss = val_loss + w1 * val_loss1 + w2 * r_loss\n",
    "            LOSS.append(val_loss.item())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            _, _, _, x = model()\n",
    "\n",
    "            # logistic regression model\n",
    "            train_x = (x[index_train[tr]]).cpu().detach().numpy()\n",
    "            train_y = Y[index_train[tr]].cpu().numpy()\n",
    "\n",
    "            val_x = x[index_train[val]].cpu().detach().numpy()\n",
    "            val_y = Y[index_train[val]].cpu().numpy().reshape(-1)\n",
    "\n",
    "            regr = linear_model.LogisticRegression(max_iter=10000)\n",
    "            regr.fit(train_x, train_y.ravel())\n",
    "            pre = regr.predict_proba(val_x)\n",
    "            pre = pre[:,1]\n",
    "            y_pred = regr.predict(val_x)\n",
    "\n",
    "            auprc = metrics.average_precision_score(val_y, pre)\n",
    "            auc = metrics.roc_auc_score(val_y, pre)   \n",
    "\n",
    "            AUC.append(auc)\n",
    "            AUPRC.append(auprc)\n",
    "        np.savetxt(f\"result/alpha/embedding_loss_{alpha}_fold{num_fold}.txt\", LOSS, delimiter=\"\\t\")\n",
    "        \n",
    "    print(f'Voi alpha = {alpha}: mean(AUC) = {mean(AUC)}')\n",
    "    print(f'Voi alpha = {alpha}: mean(AUPRC) = {mean(AUPRC)}')\n",
    "\n",
    "    np.savetxt(f\"result/alpha/AUC_test_{alpha}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/alpha/AUPRC_test_{alpha}.txt\", AUPRC, delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện và lưu tham số mô hình\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)  # numpy module.\n",
    "random.seed(seed)  # python random module.\n",
    "torch.manual_seed(seed)\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "path = \"./data/luad/\"\n",
    "EPOCH = 1000\n",
    "w1 = 0.1\n",
    "w2 = 0.015\n",
    "alpha = 0.1\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0 \n",
    "threshold = 0.7\n",
    "\n",
    "network1, network2, l_feature, r_feature, pos_edge, pos_edge1, Y = load_data(path, threshold_sim = threshold)\n",
    "\n",
    "model = Net(l_feature, r_feature, network1, network2, 1, 19, 256, 128, pos_edge, pos_edge1, alpha=alpha).to(device)     # hop=1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(1, EPOCH + 1)):\n",
    "\n",
    "    pred, pred1, r_loss, _ = model()\n",
    "    loss = F.binary_cross_entropy_with_logits(pred[index_train], Y[index_train])\n",
    "    loss1 = F.binary_cross_entropy_with_logits(pred1[index_train], Y[index_train])\n",
    "    loss = loss + w1 * loss1 + w2 * r_loss\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "torch.save(model.state_dict(), 'result/model/model.pth')\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    _, _, _, x = model()\n",
    "    x1 = x.cpu().detach().numpy()\n",
    "    np.savetxt(\"result/model/gene.txt\", x1, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chọn phương pháp cân bằng nhãn\n",
    "Do kết quả thu được chênh lệch ko đáng kể. Sử dụng thêm SMOTE sẽ làm phức tạp thêm mô hình. Vậy lên em chỉ sủ dụng RandomOverSampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_balancing(x_train, y_train, num_ovesampling = None):\n",
    "    # n_random: so luong mau sau khi RandomOverSampling\n",
    "    if num_ovesampling != None:\n",
    "        random_over_sampler = RandomOverSampler(sampling_strategy={1: num_ovesampling}, random_state=10)\n",
    "        x_resampled_1, y_resampled_1 = random_over_sampler.fit_resample(x_train, y_train)\n",
    "\n",
    "        smote = SMOTE()\n",
    "        x_resampled_2, y_resampled_2 = smote.fit_resample(x_resampled_1, y_resampled_1)\n",
    "    else: \n",
    "        random_over_sampler = RandomOverSampler(random_state=10)\n",
    "        x_resampled_2, y_resampled_2 = random_over_sampler.fit_resample(x_train, y_train)\n",
    "\n",
    "    return x_resampled_2, y_resampled_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set: \n",
      "+ size:  1892\n",
      "Test Set: \n",
      "+ size:  474\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_csv('result/model/gene.txt', header=None, sep=',')\n",
    "y = label\n",
    "\n",
    "print('Train Set: ')\n",
    "x_train = x.iloc[index_train, :]\n",
    "x_train = x_train.to_numpy()\n",
    "y_train = label_train\n",
    "print('+ size: ', len(x_train))\n",
    "\n",
    "print('Test Set: ')\n",
    "x_test = x.iloc[index_test, :]\n",
    "x_test = x_test.to_numpy()\n",
    "y_test = label_test\n",
    "print('+ size: ', len(x_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng mẫu lấy bằng RandomOverSampling:  1000\n",
      "+ AUC:  0.977755999909061\n",
      "+ AUPRC:  0.8836281480620364\n",
      "Số lượng mẫu lấy bằng RandomOverSampling:  1200\n",
      "+ AUC:  0.9777975601073241\n",
      "+ AUPRC:  0.8849974317137701\n",
      "Số lượng mẫu lấy bằng RandomOverSampling:  1500\n",
      "+ AUC:  0.9779418194582572\n",
      "+ AUPRC:  0.8848967821051148\n",
      "Số lượng mẫu lấy bằng RandomOverSampling:  None\n",
      "+ AUC:  0.9780557312404563\n",
      "+ AUPRC:  0.8859680465730053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "MAX_ITER = 10000\n",
    "fixed_solver = 'lbfgs'\n",
    "fixed_C = 2\n",
    "fixed_penalty = 'l2'\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "num_oversampling = [1000, 1200, 1500, None]\n",
    "for n in num_oversampling:\n",
    "    print('Số lượng mẫu lấy bằng RandomOverSampling: ', n)\n",
    "    lg = LogisticRegression(solver=fixed_solver, penalty=fixed_penalty, C=fixed_C, max_iter=MAX_ITER, random_state=42)\n",
    "\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "\n",
    "    for train_index, val_index in kf.split(x_train, y_train):\n",
    "\n",
    "        re_x, re_y = sample_balancing(x_train[train_index], y_train[train_index], num_ovesampling=n)\n",
    "\n",
    "        lg.fit(re_x, re_y)\n",
    "        y_pred = lg.predict(x_train[val_index])\n",
    "        pred = lg.predict_proba(x_train[val_index])[:, 1]\n",
    "\n",
    "        AUC.append(metrics.roc_auc_score(y_train[val_index], pred))\n",
    "        AUPRC.append(metrics.average_precision_score(y_train[val_index], pred))\n",
    "\n",
    "\n",
    "    np.savetxt(f\"result/resampling/AUC_test_{n}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/resampling/AUPRC_test_{n}.txt\", AUPRC, delimiter=\"\\t\")\n",
    "\n",
    "    print('+ AUC: ', mean(AUC))\n",
    "    print('+ AUPRC: ', mean(AUPRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa mô hình LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình Logistic Regression có 3 siêu tham số cần chọn đó là phương pháp hiệu chỉnh, hệ số hiệu chỉnh và thuật toán tối ưu. Với mỗi thuật toán tối ưu sẽ hỗ trợ một số phương pháp hiệu chỉnh khác nhau. Trong đó:\n",
    "\n",
    "### Thuật toán tối ưu:\n",
    "\n",
    "- **Newton's method** (`'l2'`, `'none'`): Thuật toán này cũng hướng tới việc xấp xỉ hàm loss bậc 2 tương tự như Gradient Descent, nhưng khác ở chỗ nó sử dụng phép xấp xỉ bậc 2, tại mỗi bước nó sẽ xấp xỉ hàm số f(x) bằng 1 hàm số bậc 2 quanh x và tiến 1 bước về cực đại hoặc cực tiểu của hàm đó.\n",
    "- **Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm** (`'l2'`, `'none'`): Tương tự như Newton's method nhưng ma trận Hessian được tính gần đúng bằng cách đánh giá đạo hàm. Phương pháp này rất tốt cho tập dữ liệu nhỏ.\n",
    "- **A Library for Large Linear Classification** (`'l1'`, `'l2'`): Là một phương pháp linear classification mà hỗ trợ Logistic Regression và linear SVM.\n",
    "- **Stochastic Average Gradient** (`'l2'`, `'none'`): Phương pháp SAG thực hiện tối ưu hóa tổng của một số hữu hạn các hàm lỗi trơn.\n",
    "- **SAGA** (`'elasticnet'`, `'l2'`, `'l1'`, `'none'`): Là một biến thể của SAG nhưng hỗ trợ L1, nó hoạt động rất tốt với những bộ dữ liệu rất lớn.\n",
    "\n",
    "### Phương pháp hiệu chỉnh\n",
    "\n",
    "- **none**: Không hiệu chỉnh\n",
    "- **l1**: Hiệu chỉnh L1\n",
    "- **l2**: Hiệu chỉnh L2\n",
    "- **elasticnet**: Kết hợp giữa L1 và L2\n",
    "\n",
    "### Nghịch đảo của hệ số hiệu chỉnh (C) - C càng nhỏ, hiệu chỉnh càng mạnh:\n",
    "\n",
    "\\begin{equation}\n",
    "C \\in \\{1, 2, 5, 10, 16\\} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tối ưu hóa thuật toán tối ưu\n",
    "cố định phương pháp hiệu chình là l2 và C = 2, thay đổi thuật toán tối ưu  \n",
    "===> Chọn newton-cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thuật toán tối ưu:  newton-cg\n",
      "+ AUC:  0.9780557312404563\n",
      "+ AUPRC:  0.8859680465730053\n",
      "Thuật toán tối ưu:  lbfgs\n",
      "+ AUC:  0.9780557312404563\n",
      "+ AUPRC:  0.8859680465730053\n",
      "Thuật toán tối ưu:  liblinear\n",
      "+ AUC:  0.9780718821924932\n",
      "+ AUPRC:  0.8859137996142314\n",
      "Thuật toán tối ưu:  sag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ AUC:  0.978038215037969\n",
      "+ AUPRC:  0.885818827105604\n",
      "Thuật toán tối ưu:  saga\n",
      "+ AUC:  0.9780557312404563\n",
      "+ AUPRC:  0.8857948336237252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "MAX_ITER = 10000\n",
    "solvers = {'newton-cg':['l2', 'none'], 'lbfgs':['l2', 'none'], 'liblinear':['l1', 'l2'], 'sag':['l2', 'none'], 'saga':['elasticnet', 'l1', 'l2', 'none']}\n",
    "fixed_C = 2\n",
    "fixed_penalty = 'l2'\n",
    "num_oversampling = None\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for s in solvers.keys():\n",
    "    print('Thuật toán tối ưu: ', s)\n",
    "    lg = LogisticRegression(solver=s, penalty=fixed_penalty, C=fixed_C, max_iter=MAX_ITER, random_state=42)\n",
    "\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "\n",
    "    for train_index, val_index in kf.split(x_train, y_train):\n",
    "\n",
    "        re_x, re_y = sample_balancing(x_train[train_index], y_train[train_index], num_ovesampling=n)\n",
    "\n",
    "        lg.fit(re_x, re_y)\n",
    "        y_pred = lg.predict(x_train[val_index])\n",
    "        pred = lg.predict_proba(x_train[val_index])[:, 1]\n",
    "\n",
    "        AUC.append(metrics.roc_auc_score(y_train[val_index], pred))\n",
    "        AUPRC.append(metrics.average_precision_score(y_train[val_index], pred))\n",
    "\n",
    "\n",
    "    np.savetxt(f\"result/solver/AUC_test_{s}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/solver/AUPRC_test_{s}.txt\", AUPRC, delimiter=\"\\t\")\n",
    "\n",
    "    print('+ AUC: ', mean(AUC))\n",
    "    print('+ AUPRC: ', mean(AUPRC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tối ưu hệ số hiệu chỉnh\n",
    "===> C = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hệ số hiệu chỉnh:  1\n",
      "+ AUC:  0.9776141300320365\n",
      "+ AUPRC:  0.8848543644551904\n",
      "Hệ số hiệu chỉnh:  2\n",
      "+ AUC:  0.9780557312404563\n",
      "+ AUPRC:  0.8859680465730053\n",
      "Hệ số hiệu chỉnh:  5\n",
      "+ AUC:  0.9784593156109433\n",
      "+ AUPRC:  0.8853231869965567\n",
      "Hệ số hiệu chỉnh:  10\n",
      "+ AUC:  0.9785671972349193\n",
      "+ AUPRC:  0.8857843934150282\n",
      "Hệ số hiệu chỉnh:  16\n",
      "+ AUC:  0.978451185047874\n",
      "+ AUPRC:  0.8850067008405111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "MAX_ITER = 10000\n",
    "solvers = 'newton-cg' \n",
    "C = [1,2,5,10,16]\n",
    "fixed_penalty = 'l2'\n",
    "num_oversampling = None\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for c in C:\n",
    "    print('Hệ số hiệu chỉnh: ', c)\n",
    "    lg = LogisticRegression(solver=solvers, penalty=fixed_penalty, C=c, max_iter=MAX_ITER, random_state=42)\n",
    "\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "\n",
    "    for train_index, val_index in kf.split(x_train, y_train):\n",
    "\n",
    "        re_x, re_y = sample_balancing(x_train[train_index], y_train[train_index], num_ovesampling=n)\n",
    "\n",
    "        lg.fit(re_x, re_y)\n",
    "        y_pred = lg.predict(x_train[val_index])\n",
    "        pred = lg.predict_proba(x_train[val_index])[:, 1]\n",
    "\n",
    "        AUC.append(metrics.roc_auc_score(y_train[val_index], pred))\n",
    "        AUPRC.append(metrics.average_precision_score(y_train[val_index], pred))\n",
    "\n",
    "\n",
    "    np.savetxt(f\"result/c/AUC_test_{c}.txt\", AUC, delimiter=\"\\t\")\n",
    "    np.savetxt(f\"result/c/AUPRC_test_{c}.txt\", AUPRC, delimiter=\"\\t\")\n",
    "\n",
    "    print('+ AUC: ', mean(AUC))\n",
    "    print('+ AUPRC: ', mean(AUPRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ AUC:  0.9001585623678646\n",
      "+ AUPRC:  0.7081993762226922\n",
      "+ F1  0.5811965811965812\n"
     ]
    }
   ],
   "source": [
    "# thử sử dụng LR trên tập Test\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "MAX_ITER = 100000\n",
    "solvers = 'newton-cg' \n",
    "C = 2\n",
    "fixed_penalty = 'l2'\n",
    "num_oversampling = None\n",
    "\n",
    "\n",
    "lg = LogisticRegression(solver=solvers, penalty=fixed_penalty, C= 2, max_iter=MAX_ITER, random_state=42)\n",
    "\n",
    "AUC = []\n",
    "AUPRC = []\n",
    "F1 = []\n",
    "\n",
    "\n",
    "re_x, re_y = sample_balancing(x_train, y_train, num_ovesampling=num_oversampling)\n",
    "\n",
    "lg.fit(re_x, re_y)\n",
    "y_pred = lg.predict(x_test)\n",
    "pred = lg.predict_proba(x_test)[:, 1]\n",
    "\n",
    "AUC.append(metrics.roc_auc_score(y_test, pred))\n",
    "AUPRC.append(metrics.average_precision_score(y_test, pred))\n",
    "F1.append(metrics.f1_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('+ AUC: ', mean(AUC))\n",
    "print('+ AUPRC: ', mean(AUPRC))\n",
    "print('+ F1 ', mean(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tối ưu hóa RandomForest\n",
    "Random forest có 2 siêu tham số cần lựa chọn là số cây trong rừng và độ đo chất lượng của phép tách.\n",
    "\n",
    "Với số cây, nhóm sẽ xét những số cây như sau:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{trees} \\in \\{10, 20, 50, 100, 150, 200, 300\\}\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "Với các độ đo, nhóm sẽ xét 2 độ đo là:\n",
    "\n",
    "- **entropy: Information Gain** - Được tính theo độ giảm của entropy ban đầu của hệ thống. Entropy càng lớn thì hệ thống càng hỗn loạn, phép tách làm giảm nhiều entropy đồng nghĩa với việc làm giảm nhiều độ hỗn loạn của thông tin trong hệ thống.\n",
    "- **gini: Gini Impurity** - Thể hiện mức độ sai khi ta chọn ngẫu nhiên 1 phần tử từ tập dữ liệu. Độ đo này đơn giản hơn nhiều so với Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tối ưu hóa số lương cây"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng cây:  10\n",
      "+ F1:  0.7887332422947595\n",
      "Số lượng cây:  20\n",
      "+ F1:  0.7880192754802132\n",
      "Số lượng cây:  50\n",
      "+ F1:  0.7891466935777759\n",
      "Số lượng cây:  100\n",
      "+ F1:  0.8034529272701989\n",
      "Số lượng cây:  150\n",
      "+ F1:  0.8040509366017731\n",
      "Số lượng cây:  200\n",
      "+ F1:  0.79660465787007\n",
      "Số lượng cây:  300\n",
      "+ F1:  0.8004430417084537\n"
     ]
    }
   ],
   "source": [
    "# Cố định độ đo là entropy, thay đổi số cây\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "trees = [10, 20, 50, 100, 150, 200, 300]\n",
    "MAX_ITER = 10000\n",
    "num_oversampling = None\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for t in trees:\n",
    "    F1 = []\n",
    "    print('Số lượng cây: ', t)\n",
    "    rf = RandomForestClassifier(n_estimators=t, criterion='entropy', random_state=42)\n",
    "\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "\n",
    "    for train_index, val_index in kf.split(x_train, y_train):\n",
    "        re_x, re_y = sample_balancing(x_train[train_index], y_train[train_index], num_ovesampling=n)\n",
    "        rf.fit(re_x, re_y)\n",
    "\n",
    "        y_pred = rf.predict(x_train[val_index])\n",
    "        F1.append(metrics.f1_score(y_train[val_index], y_pred))\n",
    "\n",
    "    np.savetxt(f\"result/randomforest/tree/F1_{t}.txt\", AUPRC, delimiter=\"\\t\")\n",
    "\n",
    "    print('+ F1: ', mean(F1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tối ưu hóa độ đo  \n",
    "===> chọn entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ đo:  entropy\n",
      "+ F1:  0.8004430417084537\n",
      "Độ đo:  gini\n",
      "+ F1:  0.7993165222768496\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "trees = 150 \n",
    "MAX_ITER = 10000\n",
    "num_oversampling = None\n",
    "criterion = ['entropy', 'gini']\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for cri in criterion:\n",
    "    F1 = []\n",
    "    print('Độ đo: ', cri)\n",
    "    rf = RandomForestClassifier(n_estimators=t, criterion=cri, random_state=42)\n",
    "\n",
    "    AUC = []\n",
    "    AUPRC = []\n",
    "\n",
    "    for train_index, val_index in kf.split(x_train, y_train):\n",
    "        re_x, re_y = sample_balancing(x_train[train_index], y_train[train_index], num_ovesampling=n)\n",
    "        rf.fit(re_x, re_y)\n",
    "\n",
    "        y_pred = rf.predict(x_train[val_index])\n",
    "        F1.append(metrics.f1_score(y_train[val_index], y_pred))\n",
    "\n",
    "    np.savetxt(f\"result/randomforest/cri/F1_{cri}.txt\", AUPRC, delimiter=\"\\t\")\n",
    "\n",
    "    print('+ F1: ', mean(F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So sanh LR va RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 LR:  0.6263725330918429\n",
      "Mean F1 RF:  0.7880192754802132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "MAX_ITER = 100000\n",
    "num_oversampling = None\n",
    "\n",
    "# sieu tham so mo hinh LR\n",
    "solvers = 'newton-cg' \n",
    "C = 2\n",
    "fixed_penalty = 'l2'\n",
    "\n",
    "# sieu tham so mo hinh RandomForest\n",
    "trees = 20 \n",
    "criterion = 'entropy'\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "lg = LogisticRegression(solver=solvers, C=2, penalty=fixed_penalty, max_iter=MAX_ITER, random_state=42)\n",
    "rf = RandomForestClassifier(n_estimators=trees, criterion=criterion, random_state=42)\n",
    "\n",
    "F1_LR = []\n",
    "F1_RF = []\n",
    "\n",
    "for train_index, val_index in kf.split(x_train, y_train):\n",
    "    re_x, re_y = sample_balancing(x_train[train_index], y_train[train_index], num_ovesampling=n)\n",
    "\n",
    "    # training\n",
    "    lg.fit(re_x, re_y)\n",
    "    rf.fit(re_x, re_y)\n",
    "\n",
    "    y_pred1 = lg.predict(x_train[val_index])\n",
    "    y_pred2 = rf.predict(x_train[val_index])\n",
    "    F1_LR.append(metrics.f1_score(y_train[val_index], y_pred1))\n",
    "    F1_RF.append(metrics.f1_score(y_train[val_index], y_pred2))\n",
    "\n",
    "np.savetxt(f\"result/so_sanh/F1_LR.txt\", F1_LR, delimiter=\"\\t\")\n",
    "np.savetxt(f\"result/so_sanh/F1_RF.txt\", F1_RF, delimiter=\"\\t\")\n",
    "\n",
    "print('Mean F1 LR: ', mean(F1_LR))\n",
    "print('Mean F1 RF: ', mean(F1_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6279069767441862\n",
      "0.9324894514767933\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "index_train, index_test, label_train, label_test = train_test_split(index, label, test_size=0.2, shuffle=True, random_state=10)\n",
    "\n",
    "\n",
    "trees = 150\n",
    "criterion = 'entropy'\n",
    "rf = RandomForestClassifier(n_estimators=trees, criterion=criterion, random_state=10, min_samples_split=3)\n",
    "num_oversampling = None\n",
    "\n",
    "random_over_sampler = RandomOverSampler(random_state=10, sampling_strategy='auto')\n",
    "re_x, re_y = random_over_sampler.fit_resample(x_train, y_train)\n",
    "rf.fit(re_x, re_y)\n",
    "y_pred = rf.predict(x_test)\n",
    "print(metrics.f1_score(y_test, y_pred))\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "474"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "# plt.savefig('./images/conf_matrix.png', facecolor='whitesmoke')\n",
    "len(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
